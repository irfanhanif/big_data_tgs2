{"cells":[{"cell_type":"code","source":["import os\nimport sys\nimport itertools\nfrom math import sqrt\nfrom operator import add\nfrom os.path import join, isfile, dirname\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.mllib.recommendation import ALS"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["def parseRating(line):\n  return long(line['timestamp']) % 10, (int(line['userId']), int(line['movieId']), float(line['rating']))\n  return line\n\ndef parseMovie(line):\n  return int(line['movieId']), line['movieTitle']\n\ndef loadRatings(line):\n  return int(line['userId']), int(line['movieId']), float(line['rating'])\n\ndef computeRmse(model, data, n):\n  predictions = model.predictAll(data.map(lambda x: (x[0], x[1])))\n  predictionsAndRatings = predictions.map(lambda x: ((x[0], x[1]), x[2])) \\\n    .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \\\n    .values()\n  return sqrt(predictionsAndRatings.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["myRatings = sqlContext.sql(\"select userId, movieId, rating from personalrating\").rdd.map(loadRatings)\nratings = sqlContext.sql(\"select * from rating\").rdd.map(parseRating)\nmovies = dict(sqlContext.sql(\"select * from movie\").rdd.map(parseMovie).collect())\n\nnumRatings = ratings.count()\nnumUsers = ratings.values().map(lambda r: r[0]).distinct().count()\nnumMovies = ratings.values().map(lambda r: r[1]).distinct().count()\n\nprint \"Got %d ratings from %d users on %d movies.\" % (numRatings, numUsers, numMovies)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["numPartitions = 4\ntraining = ratings.filter(lambda x: x[0] < 6) \\\n  .values() \\\n  .union(myRatings) \\\n  .repartition(numPartitions) \\\n  .cache()\n\nvalidation = ratings.filter(lambda x: x[0] >= 6 and x[0] < 8) \\\n  .values() \\\n  .repartition(numPartitions) \\\n  .cache()\n\ntest = ratings.filter(lambda x: x[0] >= 8).values().cache()\n\nnumTraining = training.count()\nnumValidation = validation.count()\nnumTest = test.count()\n\nprint \"Training: %d, validation: %d, test: %d\" % (numTraining, numValidation, numTest)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["ranks = [8, 12]\nlambdas = [0.1, 10.0]\nnumIters = [10, 20]\nbestModel = None\nbestValidationRmse = float(\"inf\")\nbestRank = 0\nbestLambda = -1.0\nbestNumIter = -1\n\nfor rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n    model = ALS.train(training, rank, numIter, lmbda)\n    validationRmse = computeRmse(model, validation, numValidation)\n    print \"RMSE (validation) = %f for the model trained with \" % validationRmse + \\\n          \"rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter)\n    if (validationRmse < bestValidationRmse):\n        bestModel = model\n        bestValidationRmse = validationRmse\n        bestRank = rank\n        bestLambda = lmbda\n        bestNumIter = numIter\n\ntestRmse = computeRmse(bestModel, test, numTest)\n\n# evaluate the best model on the test set\nprint \"The best model was trained with rank = %d and lambda = %.1f, \" % (bestRank, bestLambda) \\\n  + \"and numIter = %d, and its RMSE on the test set is %f.\" % (bestNumIter, testRmse)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["meanRating = training.union(validation).map(lambda x: x[2]).mean()\nbaselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest)\nimprovement = (baselineRmse - testRmse) / baselineRmse * 100\nprint \"The best model improves the baseline by %.2f\" % (improvement) + \"%.\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import Row\n\nmyRatingsArr = myRatings.collect()\nmyRatedMovieIds = set([x[1] for x in myRatingsArr])\ncandidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds])\npredictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect()\nrecommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]\n\nrec_disp = []\nprint \"Movies recommended for you:\"\nfor i in xrange(len(recommendations)):\n  print (\"%2d: %s\" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore')\n  rec_disp.append((Row(key=movies[recommendations[i][1]], value=recommendations[i][2])))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["rec = []\nfor i in range(10):\n  rec.append(Row(key=movies[recommendations[i][1]], value=recommendations[i][2]))\ndisplay(rec)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["rat = sqlContext.sql(\"select userId from rating\").rdd.map(tuple).collect()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["user_top = []\nfor row in rat:\n  if row[0] in user_rat:\n    user_rat[row[0]] += 1\n  else:\n    user_rat[row[0]] = 1"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from operator import itemgetter\nfrom pyspark.sql import Row\nlist = []\nfor key in user_rat:\n  list.append([key, user_rat[key]])\nlist = sorted(list, key=itemgetter(1))\nlist = map(lambda x: Row(Id=x[0],Count=x[1]), list)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(list)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["recTable = sqlContext.createDataFrame(sc.parallelize(rec_disp))\nrecTable.registerTempTable(\"recommendations\")\ndisplay(sqlContext.sql(\"select * from recommendations\"))"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"CF","notebookId":2363174356953332},"nbformat":4,"nbformat_minor":0}
